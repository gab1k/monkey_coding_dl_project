{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gab1k/Desktop/monkey_coding_dl_project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to /Users/gab1k/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используем устройство: cpu\n",
      "Размер обучающей выборки до обрезания: 25000\n",
      "Размер валидационной выборки до обрезания: 12500\n",
      "Размер тестовой выборки до обрезания: 12500\n",
      "---------------------------------------\n",
      "Итоговый размер обучающей выборки: 10000\n",
      "Итоговый размер валидационной выборки: 5000\n",
      "Итоговый размер тестовой выборки: 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 300, 64]),\n",
       " torch.Size([10000]),\n",
       " torch.Size([5000, 300, 64]),\n",
       " torch.Size([5000]),\n",
       " torch.Size([5000, 300, 64]),\n",
       " torch.Size([5000]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    word_tokenize(\"test\")\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "MAX_SEQ_LEN = 300 # Максимальная длина последовательности токенов (среднее количество ~1300, медиана ~1000). Будем надеятся что в первых 300 токенах достаточно информации чтобы что-то выучить, но мне не хватает вычислительной мощности сделать больше\n",
    "INPUT_DIM_FOR_MODEL = 64\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используем устройство: {DEVICE}\")\n",
    "imdb_dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "df_train = pd.DataFrame(imdb_dataset[\"train\"])\n",
    "df_test = pd.DataFrame(imdb_dataset[\"test\"]) \n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=42, stratify=df_test['label'])\n",
    "print(f\"Размер обучающей выборки до обрезания: {len(df_train)}\")\n",
    "print(f\"Размер валидационной выборки до обрезания: {len(df_test)}\")\n",
    "print(f\"Размер тестовой выборки до обрезания: {len(df_test)}\\n---------------------------------------\")\n",
    "\n",
    "df_train, df_val, df_test = df_train.sample(n=10000, random_state=42), df_val.sample(n=5000, random_state=42), df_test.sample(n=5000, random_state=42) # обрежем выборки, иначе модель обучается слишком долго\n",
    "\n",
    "print(f\"Итоговый размер обучающей выборки: {len(df_train)}\")\n",
    "print(f\"Итоговый размер валидационной выборки: {len(df_test)}\")\n",
    "print(f\"Итоговый размер тестовой выборки: {len(df_test)}\")\n",
    "\n",
    "def tokenize_text_data(texts_list): # токенизация\n",
    "    tokenized_texts = []\n",
    "    for text in texts_list:\n",
    "        tokens = word_tokenize(text.lower()) # из nltk\n",
    "        tokenized_texts.append(tokens)\n",
    "    return tokenized_texts\n",
    "\n",
    "def build_vocabulary(list_of_tokenized_texts): # словари token -> id, id -> token\n",
    "    all_tokens_flat = []\n",
    "    for tokens in list_of_tokenized_texts:\n",
    "        all_tokens_flat.extend(tokens)\n",
    "    \n",
    "    word_counts = Counter(all_tokens_flat)\n",
    "    most_common_words = word_counts.most_common()\n",
    "    \n",
    "    word_to_id = {'<PAD>': 0, '<UNK>': 1}\n",
    "    id_to_word = {0: '<PAD>', 1: '<UNK>'}\n",
    "    \n",
    "    for index, (word, _) in enumerate(most_common_words):\n",
    "        current_id = index + 2\n",
    "        word_to_id[word] = current_id\n",
    "        id_to_word[current_id] = word\n",
    "        \n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def convert_tokens_to_ids(list_of_tokenized_texts, word_to_id_map, max_seq_l): # [token] -> [ids]\n",
    "    sequences_of_ids = []\n",
    "    for tokens in list_of_tokenized_texts:\n",
    "        ids = [word_to_id_map.get(token, word_to_id_map['<UNK>']) for token in tokens]\n",
    "        if len(ids) < max_seq_l:\n",
    "            ids.extend([word_to_id_map['<PAD>']] * (max_seq_l - len(ids)))\n",
    "        else:\n",
    "            ids = ids[:max_seq_l]\n",
    "        sequences_of_ids.append(ids)\n",
    "    return np.array(sequences_of_ids)\n",
    "\n",
    "train_texts_tokenized = tokenize_text_data(df_train['text'].tolist())\n",
    "val_texts_tokenized = tokenize_text_data(df_val['text'].tolist())\n",
    "test_texts_tokenized = tokenize_text_data(df_test['text'].tolist())\n",
    "\n",
    "main_vocab, main_id2token = build_vocabulary(train_texts_tokenized)\n",
    "\n",
    "X_train_ids = convert_tokens_to_ids(train_texts_tokenized, main_vocab, MAX_SEQ_LEN)\n",
    "X_val_ids = convert_tokens_to_ids(val_texts_tokenized, main_vocab, MAX_SEQ_LEN)\n",
    "X_test_ids = convert_tokens_to_ids(test_texts_tokenized, main_vocab, MAX_SEQ_LEN)\n",
    "\n",
    "y_train_labels = torch.tensor(df_train['label'].values, dtype=torch.long)\n",
    "y_val_labels = torch.tensor(df_val['label'].values, dtype=torch.long)\n",
    "y_test_labels = torch.tensor(df_test['label'].values, dtype=torch.long)\n",
    "pre_embedding_layer = nn.Embedding(len(main_vocab), INPUT_DIM_FOR_MODEL, padding_idx=main_vocab['<PAD>'])\n",
    "\n",
    "with torch.no_grad(): # Без этого во время обучения модель падает, тк пытается сделать backward по\n",
    "    X_train_final = pre_embedding_layer(torch.tensor(X_train_ids, dtype=torch.long)).to(torch.float32)\n",
    "    X_val_final = pre_embedding_layer(torch.tensor(X_val_ids, dtype=torch.long)).to(torch.float32)\n",
    "    X_test_final = pre_embedding_layer(torch.tensor(X_test_ids, dtype=torch.long)).to(torch.float32)\n",
    "\n",
    "X_train_final.shape, y_train_labels.shape, X_val_final.shape,  y_val_labels.shape, X_test_final.shape, y_test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
