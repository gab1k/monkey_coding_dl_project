{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "from src.data_utils.config import DatasetConfig\n",
    "from src.data_utils.dataset_params import DatasetName\n",
    "from src.data_utils.dataset_generator import DatasetGenerator\n",
    "from src.models.models import TransformerClassifier, CustomMambaClassifier, LSTMClassifier\n",
    "\n",
    "MAX_SEQ_LEN = 300\n",
    "EMBEDDING_DIM = 128\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 7e-5 # уменьшили lr: 1e-4 -> 7e-5\n",
    "NUM_EPOCHS = 20 # подняли количество эпох: 5 -> 20\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "SAVE_DIR = \"../pretrained_comparison\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = DatasetConfig(\n",
    "    load_from_disk=True,\n",
    "    path_to_data=\"../datasets\",\n",
    "    train_size=25000,  # увеличили количество сэмплов\n",
    "    val_size=12500,\n",
    "    test_size=12500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator = DatasetGenerator(DatasetName.IMDB, config=config)\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = generator.generate_dataset()\n",
    "VOCAB_SIZE = len(generator.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, num_epochs, device, model_name, save_path):\n",
    "    best_val_f1 = 0.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': [], 'val_f1': []}\n",
    "    \n",
    "    print(f\"--- Начало обучения модели: {model_name} на устройстве {device} ---\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "        \n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_accuracy'].append(accuracy)\n",
    "        history['val_f1'].append(f1)\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Эпоха {epoch+1}/{num_epochs} | Время: {epoch_time:.2f}с | Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {accuracy:.4f} | Val F1: {f1:.4f}\")\n",
    "\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"  -> Модель сохранена, новый лучший Val F1: {best_val_f1:.4f}\")\n",
    "            \n",
    "    print(f\"--- Обучение модели {model_name} завершено ---\")\n",
    "    return history\n",
    "\n",
    "def evaluate_on_test(model, test_loader, device, criterion):\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "            \n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "        \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    \n",
    "    return {'loss': avg_test_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataloader(X, y, batch_size, shuffle=True):\n",
    "    X_tensor = torch.as_tensor(X, dtype=torch.long)\n",
    "    y_tensor = torch.as_tensor(y, dtype=torch.long)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_loader = create_dataloader(X_train, y_train, BATCH_SIZE)\n",
    "val_loader = create_dataloader(X_val, y_val, BATCH_SIZE, shuffle=False)\n",
    "test_loader = create_dataloader(X_test, y_test, BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Начало обучения модели: CustomMamba на устройстве cuda ---\n",
      "Эпоха 1/20 | Время: 1263.97с | Train Loss: 0.6205 | Val Loss: 0.5389 | Val Acc: 0.7340 | Val F1: 0.6958\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.6958\n",
      "Эпоха 2/20 | Время: 1287.85с | Train Loss: 0.4529 | Val Loss: 0.4690 | Val Acc: 0.7791 | Val F1: 0.7605\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.7605\n",
      "Эпоха 3/20 | Время: 1206.72с | Train Loss: 0.3855 | Val Loss: 0.4334 | Val Acc: 0.8014 | Val F1: 0.7886\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.7886\n",
      "Эпоха 4/20 | Время: 1322.62с | Train Loss: 0.3327 | Val Loss: 0.4332 | Val Acc: 0.8089 | Val F1: 0.7933\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.7933\n",
      "Эпоха 5/20 | Время: 1342.46с | Train Loss: 0.2897 | Val Loss: 0.4408 | Val Acc: 0.8106 | Val F1: 0.7934\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.7934\n",
      "Эпоха 6/20 | Время: 1326.47с | Train Loss: 0.2533 | Val Loss: 0.4253 | Val Acc: 0.8189 | Val F1: 0.8056\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.8056\n",
      "Эпоха 7/20 | Время: 1354.03с | Train Loss: 0.2155 | Val Loss: 0.5618 | Val Acc: 0.7999 | Val F1: 0.7700\n",
      "Эпоха 8/20 | Время: 1343.17с | Train Loss: 0.1798 | Val Loss: 0.5466 | Val Acc: 0.8122 | Val F1: 0.7944\n",
      "Эпоха 9/20 | Время: 1346.69с | Train Loss: 0.1429 | Val Loss: 0.5632 | Val Acc: 0.8175 | Val F1: 0.8145\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.8145\n",
      "Эпоха 10/20 | Время: 1334.66с | Train Loss: 0.1070 | Val Loss: 0.8960 | Val Acc: 0.7906 | Val F1: 0.7576\n",
      "Эпоха 11/20 | Время: 1325.48с | Train Loss: 0.0772 | Val Loss: 0.8872 | Val Acc: 0.8106 | Val F1: 0.8007\n",
      "Эпоха 12/20 | Время: 1350.02с | Train Loss: 0.0427 | Val Loss: 1.2565 | Val Acc: 0.8099 | Val F1: 0.7995\n",
      "Эпоха 13/20 | Время: 1279.99с | Train Loss: 0.0237 | Val Loss: 2.0416 | Val Acc: 0.7870 | Val F1: 0.7556\n",
      "Эпоха 14/20 | Время: 1247.37с | Train Loss: 0.0196 | Val Loss: 1.9937 | Val Acc: 0.7994 | Val F1: 0.7831\n",
      "Эпоха 15/20 | Время: 1227.28с | Train Loss: 0.0100 | Val Loss: 2.4182 | Val Acc: 0.8043 | Val F1: 0.7871\n",
      "Эпоха 16/20 | Время: 1223.79с | Train Loss: 0.0101 | Val Loss: 2.5203 | Val Acc: 0.8008 | Val F1: 0.7848\n",
      "Эпоха 17/20 | Время: 1227.93с | Train Loss: 0.0145 | Val Loss: 2.6083 | Val Acc: 0.7916 | Val F1: 0.7647\n",
      "Эпоха 18/20 | Время: 1241.54с | Train Loss: 0.0054 | Val Loss: 3.1163 | Val Acc: 0.7784 | Val F1: 0.7401\n",
      "Эпоха 19/20 | Время: 1165.53с | Train Loss: 0.0112 | Val Loss: 1.7516 | Val Acc: 0.8122 | Val F1: 0.8058\n",
      "Эпоха 20/20 | Время: 1076.36с | Train Loss: 0.0042 | Val Loss: 2.2866 | Val Acc: 0.8148 | Val F1: 0.8117\n",
      "--- Обучение модели CustomMamba завершено ---\n",
      "--- Оценка лучшей модели CustomMamba на тестовых данных ---\n",
      "Результаты для CustomMamba: {'loss': 0.5644057806857559, 'accuracy': 0.82048, 'precision': 0.8318422796554009, 'recall': 0.80336, 'f1_score': 0.8173530848119811}\n",
      "------------------------------------------------------------\n",
      "--- Начало обучения модели: Lib_Transformer на устройстве cuda ---\n",
      "Эпоха 1/20 | Время: 22.34с | Train Loss: 0.6061 | Val Loss: 0.5686 | Val Acc: 0.7251 | Val F1: 0.7367\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.7367\n",
      "Эпоха 2/20 | Время: 21.66с | Train Loss: 0.4844 | Val Loss: 0.4960 | Val Acc: 0.7798 | Val F1: 0.7978\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.7978\n",
      "Эпоха 3/20 | Время: 22.25с | Train Loss: 0.4087 | Val Loss: 0.4497 | Val Acc: 0.7990 | Val F1: 0.7876\n",
      "Эпоха 4/20 | Время: 21.85с | Train Loss: 0.3748 | Val Loss: 0.4218 | Val Acc: 0.8197 | Val F1: 0.8176\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.8176\n",
      "Эпоха 5/20 | Время: 22.06с | Train Loss: 0.3427 | Val Loss: 0.4145 | Val Acc: 0.8250 | Val F1: 0.8196\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.8196\n",
      "Эпоха 6/20 | Время: 22.33с | Train Loss: 0.3163 | Val Loss: 0.4095 | Val Acc: 0.8240 | Val F1: 0.8358\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.8358\n",
      "Эпоха 7/20 | Время: 22.08с | Train Loss: 0.2900 | Val Loss: 0.3892 | Val Acc: 0.8363 | Val F1: 0.8442\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.8442\n",
      "Эпоха 8/20 | Время: 21.89с | Train Loss: 0.2669 | Val Loss: 0.4275 | Val Acc: 0.8172 | Val F1: 0.8369\n",
      "Эпоха 9/20 | Время: 22.39с | Train Loss: 0.2430 | Val Loss: 0.3763 | Val Acc: 0.8442 | Val F1: 0.8482\n",
      "  -> Модель сохранена, новый лучший Val F1: 0.8482\n",
      "Эпоха 10/20 | Время: 21.92с | Train Loss: 0.2173 | Val Loss: 0.4122 | Val Acc: 0.8222 | Val F1: 0.8043\n",
      "Эпоха 11/20 | Время: 24.34с | Train Loss: 0.1962 | Val Loss: 0.3785 | Val Acc: 0.8402 | Val F1: 0.8388\n",
      "Эпоха 12/20 | Время: 23.54с | Train Loss: 0.1743 | Val Loss: 0.3845 | Val Acc: 0.8387 | Val F1: 0.8448\n",
      "Эпоха 13/20 | Время: 21.97с | Train Loss: 0.1507 | Val Loss: 0.3881 | Val Acc: 0.8382 | Val F1: 0.8361\n",
      "Эпоха 14/20 | Время: 22.07с | Train Loss: 0.1292 | Val Loss: 0.4008 | Val Acc: 0.8400 | Val F1: 0.8435\n",
      "Эпоха 15/20 | Время: 22.17с | Train Loss: 0.1111 | Val Loss: 0.4491 | Val Acc: 0.8274 | Val F1: 0.8404\n",
      "Эпоха 16/20 | Время: 22.06с | Train Loss: 0.0930 | Val Loss: 0.4288 | Val Acc: 0.8372 | Val F1: 0.8427\n",
      "Эпоха 17/20 | Время: 22.01с | Train Loss: 0.0802 | Val Loss: 0.5088 | Val Acc: 0.8262 | Val F1: 0.8391\n",
      "Эпоха 18/20 | Время: 23.16с | Train Loss: 0.0649 | Val Loss: 0.4934 | Val Acc: 0.8307 | Val F1: 0.8398\n",
      "Эпоха 19/20 | Время: 24.57с | Train Loss: 0.0517 | Val Loss: 0.5197 | Val Acc: 0.8321 | Val F1: 0.8402\n",
      "Эпоха 20/20 | Время: 22.65с | Train Loss: 0.0471 | Val Loss: 0.5762 | Val Acc: 0.8297 | Val F1: 0.8348\n",
      "--- Обучение модели Lib_Transformer завершено ---\n",
      "--- Оценка лучшей модели Lib_Transformer на тестовых данных ---\n",
      "Результаты для Lib_Transformer: {'loss': 0.3755408748412681, 'accuracy': 0.84224, 'precision': 0.8261665141811528, 'recall': 0.86688, 'f1_score': 0.8460337289194254}\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Итоговая таблица сравнения моделей на тестовых данных ---\n",
      "                     loss  accuracy  precision   recall  f1_score\n",
      "CustomMamba      0.564406   0.82048   0.831842  0.80336  0.817353\n",
      "Lib_Transformer  0.375541   0.84224   0.826167  0.86688  0.846034\n"
     ]
    }
   ],
   "source": [
    "model_configs = {\n",
    "    \"CustomMamba\": {\n",
    "        \"class\": CustomMambaClassifier,\n",
    "        \"params\": {'vocab_size': VOCAB_SIZE, 'd_model': EMBEDDING_DIM, 'd_state': 8, \n",
    "                   'd_conv': 4, 'num_layers': 2, 'num_classes': NUM_CLASSES},\n",
    "    },\n",
    "\n",
    "    \"Lib_Transformer\": {\n",
    "        \"class\": TransformerClassifier,\n",
    "        \"params\": {'vocab_size': VOCAB_SIZE, 'embed_dim': EMBEDDING_DIM, 'num_heads': 8, \n",
    "                   'num_layers': 4, 'num_classes': NUM_CLASSES, 'max_seq_len': MAX_SEQ_LEN},\n",
    "        # num_layers: 2 -> 4\n",
    "        # num_heads: 4 -> 8\n",
    "    },\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for model_name, config in model_configs.items():\n",
    "\n",
    "    model_path = os.path.join(SAVE_DIR, f\"best_model_{model_name.lower()}.pth\")\n",
    "    \n",
    "    model = config['class'](**config['params']).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_and_evaluate(\n",
    "        model=model, train_loader=train_loader, val_loader=val_loader,\n",
    "        optimizer=optimizer, criterion=criterion, num_epochs=NUM_EPOCHS,\n",
    "        device=DEVICE, model_name=model_name, save_path=model_path\n",
    "    )\n",
    "    \n",
    "    print(f\"--- Оценка лучшей модели {model_name} на тестовых данных ---\")\n",
    "    if os.path.exists(model_path):\n",
    "        best_model = config['class'](**config['params']).to(DEVICE)\n",
    "        best_model.load_state_dict(torch.load(model_path))\n",
    "        test_metrics = evaluate_on_test(best_model, test_loader, DEVICE, criterion)\n",
    "        results[model_name] = test_metrics\n",
    "        print(f\"Результаты для {model_name}: {test_metrics}\")\n",
    "    else:\n",
    "        print(f\"Файл лучшей модели для {model_name} не найден. Пропускаем оценку.\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "if results:\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(\"\\n\\n--- Итоговая таблица сравнения моделей на тестовых данных ---\")\n",
    "    print(results_df.to_string())\n",
    "else:\n",
    "    print(\"Не удалось получить результаты ни для одной модели.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что transformer сильно выигрывает у мамбы как и по времени, так и по качеству. Будем обучать и инферить его "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monkey-coding-dl-project-F4QJzkF_-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
